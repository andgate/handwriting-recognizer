{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1410eba6",
   "metadata": {},
   "source": [
    "This notebook is used to for to train and build the model that our webapp will ultimately serve to users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12906043",
   "metadata": {},
   "source": [
    "Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b305f06",
   "metadata": {},
   "source": [
    "Define neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bcd6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input size (1, 28, 28)\n",
    "            # First convolution block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1), # (16, 26, 26) - kernel size of 3 reduces spatial dimensions by 2\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2),  # (16, 13, 13) - max-pooling with kernel size of 2 reduces spatial dimensions by half\n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1), # (32, 24, 24) - kernel size of 3 reduces spatial dimensions by 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (64, 12, 12) - max-pooling with kernel size of 2 reduces spatial dimensions by half\n",
    "            # Linear head\n",
    "            nn.Dropout2d(0.7),\n",
    "            nn.Flatten(),\n",
    "            # Simple linear layer in linear head\n",
    "            nn.Linear(64*12*12, 128),\n",
    "            nn.ReLU(),\n",
    "            # Final linear layer in head\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10), # compress 128 pixels (12*12) into 10 outputs for each digit,\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429236ba",
   "metadata": {},
   "source": [
    "Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5ad413",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "data_augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    mnist_transform\n",
    "])\n",
    "\n",
    "original_train_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=data_augmentation\n",
    ")\n",
    "augmented_train_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=data_augmentation\n",
    ")\n",
    "train_dataset = ConcatDataset([original_train_dataset, augmented_train_dataset])\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=mnist_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6b86192",
   "metadata": {},
   "source": [
    "Define training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc0ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        _, pred = torch.max(output, 1)\n",
    "        total_correct += (pred == target).sum().item()\n",
    "        total_samples += data.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * total_correct / total_samples\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            total_correct += (pred == target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * total_correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7626b39e",
   "metadata": {},
   "source": [
    "Train and test the model, then plot the train and test losses per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda, train samples: 120000, test samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DigitClassifier().to(device)\n",
    "criterion = nn.NLLLoss(reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f'device: {device.type}, train samples: {len(train_loader.dataset)}, test samples: {len(test_loader.dataset)}')\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "for epoch in tqdm(range(1, num_epochs + 1), desc='Training model...'):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, criterion)\n",
    "    test_loss, test_accuracy = test(model, device, test_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # print(f'Epoch {epoch}: Average Train Loss = {np.mean(train_loss):.4f}, Average Test Loss = {np.mean(test_loss):.4f}')\n",
    "    # print(f'\\tTrain Accuracy = {train_accuracy:.4f}, Test Accuracy = {test_accuracy:.4f}')\n",
    "print('Complete.')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss per sample')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
